{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building a Better Isolation Evaluation Function\n",
    "#### Hoxie Ackerman (hoxiea@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After implementing Minimax search and Iterative-Deepening Search with Alpha-Beta pruning for my IsolationPlayers, I next tackled the problem of developing an Isolation evaluation function that outperforms the provided  `improved` evaluation function. \n",
    "\n",
    "For reference, the `improved` evaluation function for a certain board configuration is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def improved_score(game, player):\n",
    "    \"\"\"\n",
    "    The \"Improved\" evaluation function discussed in lecture that outputs a\n",
    "    score equal to the difference in the number of moves available to the\n",
    "    two players.\n",
    "    \"\"\"\n",
    "    if game.is_loser(player):\n",
    "        return float(\"-inf\")\n",
    "    if game.is_winner(player):\n",
    "        return float(\"inf\")\n",
    "    own_moves = len(game.get_legal_moves(player))\n",
    "    opp_moves = len(game.get_legal_moves(game.get_opponent(player)))\n",
    "    return float(own_moves - opp_moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To refresh our memories, the **evaluation function** is the method that an IsolationPlayer uses to score Boards that it encounters in its search tree. A useful evaluation function will consider various properties of the Board from our player's perspective, and should consistently assign higher scores to Boards in which we have a better chance of winning the game.\n",
    "\n",
    "`improved_score` does something very reasonable along these lines: Boards where we have more legal moves will score higher, as will Boards where our opponent has fewer legal moves. These are extremely reasonable Board properties to consider in a game where you lose by running out of legal moves.\n",
    "\n",
    "As Dr. Starner mentioned in Lecture 9 of the \"Project: Build a Game-Playing Agent\" videos, one can consider adjust the \"weights\" of the terms in the linear combination of `improved_score`. In particular, he proposed a modified version of `improved_score` that would essentially return `own_moves - 2 * opp_moves`. He claimed that IsolationPlayers using this evaluation function would, on average, play more aggresively, leaning more towards Boards where the opponent has fewer moves, which is ultimately how you win in this game. So that seems like a very reasonable modification to make.\n",
    "\n",
    "But is \"-2\" really the optimal coefficient for `opp_moves`? And are there other features of a Board, aside from `own_moves` and `opp_moves` that would also point our IsolationPlayers towards better board positions? Let's investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2: How Will We Know When We're Done?\n",
    "\n",
    "If the goal is to construct an evaluation function that outperforms `score_improved`, I think it's important to define what we mean by \"outperform.\" Given a new-and-possibly-improved evaluation function `contender`, it seems like I have two options for demonstrating superiority:\n",
    "\n",
    "1. Show, using the tools of math and logic, that `contender` is provably better than score_improved.\n",
    "2. Show empirically that `contender` performs better, on average, than `score_improved`.\n",
    "\n",
    "Option 2 seems much easier, so let's pursue this approach.\n",
    "\n",
    "One reason why Option 2 is easier: the project comes with `tournament.py`, which pits the **test agents** `AB_Improved` (an `AlphaBetaPlayer` using `score_improved`) and one or more `AlphaBetaPlayers` equipped custom evaluation functions against a motley gauntlet of seven opponents: an opponent that moves randomly, three `MinimaxPlayer`s using various evaluation functions, and three `AlphaBetaPlayer`s using various evaluation functions. Overall Win Rates are calculated for each test agent.\n",
    "\n",
    "So that's the plan. Let's see an example of the plan in action:\n",
    "\n",
    "In particular, suppose I had developed nine different evaluation functions using the following pseudocode algorithm:\n",
    "```\n",
    "for own_moves_weight in (1, 2, 3):\n",
    "    for opp_moves_weight in (-3, -2, -1):\n",
    "        modify improved_score to use own_moves_weight and opp_moves_weight\n",
    "```\n",
    "\n",
    "According to the plan, I should equip some `AlphaBetaPlayers` with these evaluation functions and run some tournaments. I did this, and obtained the following output, where the tuple in the test_agent's name indicates the weights used:\n",
    "\n",
    "![Effects of varying opp_moves_weight, with constant own_moves_weight = 1.](report-pics/section2/1-DONE.png)\n",
    "\n",
    "![Effects of varying opp_moves_weight, with constant own_moves_weight = 2.](report-pics/section2/2-DONE.png)\n",
    "\n",
    "![Effects of varying opp_moves_weight, with constant own_moves_weight = 3.](report-pics/section2/3-DONE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So let's see... (1, -1) actually seems like the best combination tested, with a Win Rate of 75.7%. But (3, -2) was also pretty potent, with a Win Rate of 74.3%. And in all three runs, at least one of these weighted test agents outperformed AB_Improved. So give or take some fine-tuning, we've got our improved evaluation functions, right?\n",
    "\n",
    "Maybe, but I'm not convinced, and you shouldn't be either. This is a random process (for example: `AB_Improved` won 67.1% of its games in the first and third tournaments, and only 61.4% in the second tournament). If I were to repeat this entire process with three more tournaments, the win rates could change. How can we be sure that what we're seeing in a given run is signal (an actual difference in performance between evaluation functions) and not noise (random variation)?\n",
    "\n",
    "This is, of course, a statistics problem. The randomness makes it almost impossible to say anything with certainty. But a statistical test would (ideally) let us conclude with high confidence/probability that the true win rate for one of our custom evaluation functions was higher than it was for `score_improved.`\n",
    "\n",
    "I presented this point of view in the [Udacity forums](https://discussions.udacity.com/t/sources-of-variation-in-tournaments-and-why-forfeits/268387/5?u=hha) (that this was inherently a statistical problem, and that we should be thinking about statistical tests), and it was mostly glossed over. The proposed solution to this problem was just to run more matches (i.e. increase the sample size to decrease noise levels for the means), and just eyeball the results for differences.\n",
    "\n",
    "Even if we're not going to perform a formal statistical test, it still makes sense to get a feel for the amount of variability in this process. If `AB_Improved` is always between 61.4% and 67.1% (the values we observed in the three tournaments above), for example, then a `contender` with win rates consistently in the 70+% range would seem to be outperforming `AB_Improved` with high probability.\n",
    "\n",
    "As it turns out, with 5 fair matches per matchup (i.e. 10 games per matchup, as above), there's siginificantly more variability than 61-67%. **Confession time: I didn't actually implement those nine evaluation functions. Every single one of the test agents in the above three tournaments was an `AB_Improved` agent.** So based on those results, we see that with 5 fair matches per matchup, the quantity that we're supposed to be comparing our performance against can be anywhere between about 60% and about 75%. Maybe there's even greater variability that we didn't happen to see in those three runs.\n",
    "\n",
    "That seems like quite a bit of background noise to me. And the more I thought about this, the stranger that seemed to me. After all, this is a fully observable, deterministic, static, discrete environment that we're playing in here. And the algorithms that our AIs are using (aside from the `Random` opponent, who just selects a random legal move on each turn) are almost completely deterministic. Where is all this variation coming from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3: Analysis of Variance\n",
    "\n",
    "First, let's make sure that we can really reduce the variability in Win Rates by increasing the number of fair matches (let's call that `NUM_MATCHES`). Here's a tournament with all four test agents using `score_improved` and `NUM_MATCHES` increased to 25:\n",
    "\n",
    "![Tournament, all test agents using ab_improved, NUM_MATCHES = 25](report-pics/section3/tourn1.png)\n",
    "\n",
    "So indeed, by playing more matches per matchup, the Win Rates are within just a couple percentage points of about 67%, which is the figure that's often reported in the forums for working code.\n",
    "\n",
    "The problem with this approach is that it's computationally expensive. A gridsearch in a multidimensional weight space to find good weights for the various board features in our custom evaluation function(s) could take hours or days. If we need to pay that price, then that's life, but consider this row from that output for a second:\n",
    "\n",
    "![Same evaluation function for all test agents, same starting Board configurations, same opponent, mostly deterministic search algorithms: where is that variation coming from?](report-pics/section3/tourn1-row.png)\n",
    "\n",
    "Why is there so much variation in those outcomes? As I said above, this is a fully observable, deterministic, static, discrete environment, and our agents are using what appear to be mostly deterministic search algorithms to select their moves. \n",
    "\n",
    "Well, note that these things seem to be the same (and therefore can't be causing the observed differences):\n",
    "\n",
    "* The opponent is using the same evaluation function in all four cells\n",
    "* The evaluation function used by the four test agents is exactly the same\n",
    "* They're all using the same `AlphaBetaPlayer` code, and Alpha-Beta search isn't inherently a random algorithm\n",
    "* The initial board configurations are exactly the same for each set of 50 games\n",
    "\n",
    "The only differences I could think of were:\n",
    "\n",
    "1. Differences in depth searched, due to differences in available computing resources. I started that tournament on an otherwise idling computer (with a quad-core processor) and then walked away from it while it ran, but maybe some background task was finishing up while the first test agent played, and then another background task started with the fourth agent was playing, meaning they could search less deeply in the provided time, resulting in their lower win rates.\n",
    "\n",
    "2. The single piece of randomness in my Alpha-Beta implementation: the initialization of `best_move` to a random legal move before the search begins.\n",
    "\n",
    "(This list actually used to have 3 items: I misread the code in `tournament.py` and thought that the test agents could be playing with different initial board configurations, which seemed like a huge potential source of variability in a given row. So I rewrote `tournament.py` to implement what I called \"fair rounds,\" where the same opening moves are used for all test agents' games, only to learn that it already did that. Oops. I will note that the code can be made much cleaner by using `Counter`s to count things.)\n",
    "\n",
    "Controlling for #1 would be doable: we could let the first test agent search as deeply as it could before time ran out, and then limit the rest of the test agents based on depth, not time. \n",
    "\n",
    "But controlling for #2 was laughably easy. While reading the project code, I noticed that `Board.__get_moves` first produces all legal moves in the same order every time, and then shuffles (randomizes) that list of moves before it returns it. So when I wrote my `AlphaBeta` code, I didn't bother to shuffle the result of `Board.get_legal_moves` (as many people in the forums, mentors included, were recommending), because it would just be a complication and a waste of computational resources - the list is already shuffled! Instead, I just initialized `best_move = game.get_legal_moves(self)[0]`, which gets a random legal move if `Board.__get_moves` is shuffling, and **which will return the same move every time, given the board configuration, if that \"shuffle\" line is commented out**. Commenting out that single line of code (`random.shuffle(valid_moves)` in `Board.__get_moves`) and rerunning the tournament produced the following results:\n",
    "\n",
    "![Tournament, all test agents using ab_improved, NUM_MATCHES = 25. Same best_move initialization every time, given the board configuraton.](report-pics/section3/tourn2.png)\n",
    "\n",
    "Note how much variability across the rows has decreased! The main exception to this is `Random`, but this particular opponent obviously has a source of randomness that the `MinimaxPlayer`s and `AlphaBetaPlayer`s don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Q: Why does the best_move initialization matter so much? After all, I have this sweet search algorithm - shouldn't it almost immediately find a better move and overwrite that initial best_move value?**\n",
    "\n",
    "Great question. In game situations where there are at least one decent legal move, the first decent move should result in a board with a much higher score than $-\\infty$, causing `best_move` to be overwritten. But the way that I coded it, `best_move` is only overwritten if a legal move has a score *strictly greater* than the previous `best_move`. And if it's looking bleak for our test agent, in the sense that all roads appear to lead to a loss, then `best_move` will never actually get overwritten. \n",
    "\n",
    "This is the source of many of the forfeits mentioned in the forums: if you initialize `best_move` to `(-1, -1)` and the above effect occurs, then the agent will forfeit as soon as it seems like it's out of options. But immediately forfeiting instead of playing some legal move is a highly suboptimal strategy: this apparent bleakness is a function of the probably-not-perfect evaluation function that the agent is using, so it might not even be an accurate reflection of the situation. And even if it is accurate, playing a legal move gives the opponent an opportunity to forfeit, play a bad move, etc., which could salvage the game for our agent.\n",
    "\n",
    "**Q: Is there a better way to initialize `best_move` than just picking a random legal move? It seems like it gets played a fair amount, so optimizing this could be worth it.**\n",
    "\n",
    "Probably. For example, you could initialize it to the available move that moves you closest to the center of the board, or to the available move that achieves some other short-term goal. I didn't explore this idea, but it's a good one.\n",
    "\n",
    "**Q: Does this decrease in variability hold when we reduce `NUM_MATCHES` back to something that's more computationally tractable?**\n",
    "\n",
    "It certainly seems to:\n",
    "\n",
    "![Tournament, all test agents using ab_improved, NUM_MATCHES = 5, non-random best_move initialization](report-pics/section3/num-matches-5/tourn1.png)\n",
    "\n",
    "![Tournament, all test agents using ab_improved, NUM_MATCHES = 5, non-random best_move initialization](report-pics/section3/num-matches-5/tourn5.png)\n",
    "\n",
    "\n",
    "**Q: Couldn't we make this even more computationally tractable by parallelizing the tournament? Evaluating multiple evaluation functions (in independent matches) seems like an embarassingly parallel problem.**\n",
    "\n",
    "Yes, I think we can. I played around with this idea on my own a little bit before learning that there's a [git branch dedicated to parallel tournaments](https://github.com/udacity/AIND-Isolation/tree/xMultiProcessing). I started using that, and computational time decreased significantly, as expected.\n",
    "\n",
    "**Q: So what's the plan moving forward?**\n",
    "\n",
    "* The reduction in variation achieved by initializating `best_move` to the first available legal move consistently should make it much easier to visually attribute differences between Win Rate to the evaluation functions used and not to random variation\n",
    "* To further reduce variation, let's **remove `Random` from the gauntlet of opponents**\n",
    "* This reduction in variation seems to hold even when `NUM_MATCHES` is relatively small\n",
    "* And running the tournaments in parallel makes things go faster\n",
    "\n",
    "**So let's run a bunch of tournaments with `AB_Improved` and test agents equipped with different evaluation functions to see if we can find an evaluation function that outperforms `score_improved`!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4: Running a Bunch of Tournaments\n",
    "\n",
    "With this strategy in mind, let's build some infrastructure to run those tournaments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4a: Board Features\n",
    "As hinted at in the Introductory section of this report, the evaluation functions we try will be linear combinations of various features of the Isolation board. Let's define these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def num_moves(game, player):\n",
    "    \"\"\"How many legal moves does `player` have in `game`?\"\"\"\n",
    "    return len(game.get_legal_moves(player))\n",
    "\n",
    "def num_moves_opponent(game, player):\n",
    "    \"\"\"How many legal moves does the opponent of `player` have in `game`?\"\"\"\n",
    "    return len(game.get_legal_moves(game.get_opponent(player)))\n",
    "\n",
    "def dist_from_center(game, player):\n",
    "    \"\"\"What is `player`'s Manhattan distance from the center of the board?\"\"\"\n",
    "    center_x = game.height / 2\n",
    "    center_y = game.width / 2\n",
    "    player_x, player_y = game.get_player_location(player)\n",
    "    return abs(center_x - player_x) + abs(center_y - player_y)\n",
    "\n",
    "def dist_from_opponent(game, player):\n",
    "    \"\"\"What is `player`'s Manhattan distance from its opponent?\"\"\"\n",
    "    player_x, player_y = game.get_player_location(player)\n",
    "    opponent = game.get_opponent(player)\n",
    "    opp_x, opp_y = game.get_player_location(opponent)\n",
    "    return abs(opp_x - player_x) + abs(opp_y - player_y)\n",
    "\n",
    "HEURISTICS = (num_moves, num_moves_opponent, dist_from_center, dist_from_opponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4b: Agents Using Board Features\n",
    "\n",
    "Next, we'll need a way to generate tournaments Agents that use `AlphaBetaPlayer`s, equipped with linear combinations of these board features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "from game_agent import AlphaBetaPlayer\n",
    "\n",
    "WAgent = namedtuple(\"WAgent\", [\"player\", \"name\", \"weights\"])   # tournament Agent with weights\n",
    "\n",
    "def make_AB_WAgent(weights, fns=HEURISTICS):\n",
    "    \"\"\"Make a WAgent, using a linear combo of `weights` and `fns`.\"\"\"\n",
    "    score_fn = build_weighted_fn(weights, fns)\n",
    "    name = weights_to_string(weights)\n",
    "    return WAgent(AlphaBetaPlayer(score_fn=score_fn), name, weights)\n",
    "\n",
    "# Helper functions\n",
    "def build_weighted_fn(weights, fns):\n",
    "    \"\"\"\n",
    "    Given weights and heuristic functions, create a score function that's a linear\n",
    "    combination those weights and heuristics.\n",
    "    \"\"\"\n",
    "    assert len(weights) == len(fns)\n",
    "    def score_fn(game, player):\n",
    "        if game.is_loser(player):\n",
    "            return float(\"-inf\")\n",
    "        if game.is_winner(player):\n",
    "            return float(\"inf\")\n",
    "        # skip features with weight == 0 to avoid computational time\n",
    "        return sum(w * f(game, player) for w, f in zip(weights, fns) if w)\n",
    "    return score_fn\n",
    "\n",
    "def weights_to_string(weights):\n",
    "    template = \"|\".join([\"{}\"] * len(weights))\n",
    "    return template.format(*weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4c: Which Linear Combinations?\n",
    "\n",
    "Which linear combinations of our board features should our Agents use? I think it makes sense to consider positive coefficients for `num_moves` and negative coefficients for `num_moves_opponent`. I'm not sure how `dist_from_center` and `dist_from_opponent` should be treated, so I'll consider both positive and negative coefficients, and I'll make their scales similar so that they can have an equally large impact on the board's score if they have something to say.\n",
    "\n",
    "Here are the coefficient values I'll consider for each board feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "COEFFS = (\n",
    "    (0, 1, 2, 3, 4),       # num_moves\n",
    "    (-4, -3, -2, -1, 0),   # num_moves_opponent\n",
    "    (-4, -2, 0, 2, 4),     # dist_from_center\n",
    "    (-4, -2, 0, 2, 4)      # dist_from_opponent\n",
    ")\n",
    "\n",
    "# How many combinations?\n",
    "num_combinations = reduce(mul, (len(cs) for cs in COEFFS), 1)\n",
    "num_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4d: Agent Creation, Initial Test\n",
    "\n",
    "Let's build some agents using our coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def make_all_agents():\n",
    "    for weights in product(*COEFFS):\n",
    "        yield make_AB_WAgent(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Great, we should be (almost) good to go. Let's run a small tournament to verify that this all works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Match #   Opponent    AB_Improved   2|-1|0|0     1|-2|0|0     2|-3|1|-1  \n",
      "                        Won | Lost   Won | Lost   Won | Lost   Won | Lost \n",
      "    1       MM_Open      2  |   2     2  |   2     1  |   3     2  |   2  \n",
      "    2      MM_Center     2  |   2     4  |   0     3  |   1     4  |   0  \n",
      "    3     MM_Improved    4  |   0     2  |   2     4  |   0     3  |   1  \n",
      "    4       AB_Open      3  |   1     2  |   2     2  |   2     2  |   2  \n",
      "    5      AB_Center     1  |   3     3  |   1     1  |   3     3  |   1  \n",
      "    6     AB_Improved    2  |   2     1  |   3     1  |   3     1  |   3  \n",
      "--------------------------------------------------------------------------\n",
      "           Win Rate:      58.3%        58.3%        50.0%        62.5%    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tournament import Agent, play_matches\n",
    "from game_agent import custom_score, custom_score_2, custom_score_3, MinimaxPlayer\n",
    "from sample_players import open_move_score, center_score\n",
    "\n",
    "test_agents = [\n",
    "    Agent(AlphaBetaPlayer(score_fn=improved_score), \"AB_Improved\"),\n",
    "    make_AB_WAgent((2, -1, 0, 0)),   # favor boards where we have moves\n",
    "    make_AB_WAgent((1, -2, 0, 0)),   # favor boards where opponent lacks moves\n",
    "    make_AB_WAgent((2, -3, 1, -1))   # a bit agressive, use the other features too\n",
    "]\n",
    "\n",
    "cpu_agents = [\n",
    "    Agent(MinimaxPlayer(score_fn=open_move_score), \"MM_Open\"),\n",
    "    Agent(MinimaxPlayer(score_fn=center_score), \"MM_Center\"),\n",
    "    Agent(MinimaxPlayer(score_fn=improved_score), \"MM_Improved\"),\n",
    "    Agent(AlphaBetaPlayer(score_fn=open_move_score), \"AB_Open\"),\n",
    "    Agent(AlphaBetaPlayer(score_fn=center_score), \"AB_Center\"),\n",
    "    Agent(AlphaBetaPlayer(score_fn=improved_score), \"AB_Improved\")\n",
    "]\n",
    "\n",
    "play_matches(cpu_agents, test_agents, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**NOTE:** This actually *doesn't* work with the standard parallel `tournament.py` available on Github. If you run a tournament, Python complains about being unable to pickle the score functions for our WAgents. It turns out the the `multiprocessing` library, which the parallel version of `tournament.py` uses, serializes objects using `pickle` before it queues them for processing. Unfortunately, `pickle` doesn't like the custom evaluation functions that we're equipping our custom Agents with because they're produced somewhat dynamically by higher-order function `build_weighted_fn`, and Python can only pickle functions that are defined at the top-level ([link](https://docs.python.org/3.1/library/pickle.html#what-can-be-pickled-and-unpickled)). \n",
    "\n",
    "A bit of Googling led to a relevant StackOverflow [question](https://stackoverflow.com/questions/8804830/python-multiprocessing-pickling-error), and the easiest solution to try first seemed to be using the `pathos` library instead of `multiprocessing`. `pathos` is a fork of `multiprocessing` that uses a serialization library called `dill`, which can serialize objects that `pickle` can't. This solves our problem. \n",
    "\n",
    "The next solution I would have tried would be to define a child class of `AlphaBetaPlayer` called `WeightedAlphaBetaPlayer` that was initialized with its weights vector and had our various board feature functions as class methods. `self.score` could be overrided to use these methods and the weights to compute the score for a Board. But that would have involved changing quite a bit of these report, and I'm not 100% sure that would have been pickle-able, so thankfully `pathos` solved my problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4e: Tournament Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "variables": {
     "num_combinations // 3 + 1": "209"
    }
   },
   "source": [
    "The plan is to run {{num_combinations // 3 + 1}} tournaments. (**NOTE:** That value is generated dynamically as a function of `num_combinations`, enabled by the \"Python Markdown\" [nbextension](https://github.com/ipython-contrib/jupyter_contrib_nbextensions). Cool!) \n",
    "\n",
    "And the tournaments that `tournament.py` runs by default don't actually capture their results; they print them to the terminal but then essentially throw them away, as seen above. \n",
    "\n",
    "We'd like to do the opposite: run the tournaments, probably without flooding STDOUT with thousands of lines of tournament results, and collect the results achieved by the various coefficient weight vectors to see which linear combinations of weights are outperforming `score_improved`.\n",
    "\n",
    "Fortunately, it's easy enough to modify `tournament.play_matches` to do what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def run_tournament_OLD(cpu_agents, test_agents, num_matches):\n",
    "    \"\"\"\n",
    "    Play the same fair matches between the test agent and each cpu_agent individually.\n",
    "    Returns the total number of games won by each test_agent\n",
    "    Based on code provided by Udacity in tournament.py\n",
    "    \"\"\"\n",
    "    total_wins = Counter()\n",
    "    for agent in cpu_agents:\n",
    "        wins = Counter()\n",
    "        counts = play_round(agent, test_agents, wins, num_matches)\n",
    "        del wins[agent]\n",
    "        total_wins.update(wins)\n",
    "    \n",
    "    # The keys of total_wins are the test_agent.players\n",
    "    # We'd like to group the weights with the number of wins\n",
    "    abi, custom_agents = test_agents[0], test_agents[1:]\n",
    "    abi_wins = total_wins[abi.player]\n",
    "    weight_wins = dict()\n",
    "    for ta in custom_agents:\n",
    "        ta_wins = total_wins[ta.player]\n",
    "        if ta_wins > abi_wins:\n",
    "            weight_wins[ta.weights] = ta_wins\n",
    "    \n",
    "    return weight_wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'run_tournament'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9fec0e30233d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtournament\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_tournament\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrun_tournament\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpu_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'run_tournament'"
     ]
    }
   ],
   "source": [
    "from tournament import run_tournament\n",
    "\n",
    "run_tournament(cpu_agents, test_agents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
