{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building a Better Isolation Evaluation Function\n",
    "#### Hoxie Ackerman (hoxiea@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After implementing Minimax search and Iterative-Deepening Search with Alpha-Beta pruning for my IsolationPlayers, I next tackled the problem of developing an Isolation evaluation function that outperforms the provided  `improved` evaluation function. \n",
    "\n",
    "For reference, the `improved` evaluation function for a certain board configuration is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def improved_score(game, player):\n",
    "    \"\"\"\n",
    "    The \"Improved\" evaluation function discussed in lecture that outputs a\n",
    "    score equal to the difference in the number of moves available to the\n",
    "    two players.\n",
    "    \"\"\"\n",
    "    if game.is_loser(player):\n",
    "        return float(\"-inf\")\n",
    "    if game.is_winner(player):\n",
    "        return float(\"inf\")\n",
    "    own_moves = len(game.get_legal_moves(player))\n",
    "    opp_moves = len(game.get_legal_moves(game.get_opponent(player)))\n",
    "    return float(own_moves - opp_moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To refresh our memories, the **evaluation function** is the method that an IsolationPlayer uses to score Boards that it encounters in its search tree. A useful evaluation function will consider various properties of the Board from our player's perspective, and should consistently assign higher scores to Boards in which we have a better chance of winning the game.\n",
    "\n",
    "`improved_score` does something very reasonable along these lines: Boards where we have more legal moves will score higher, as will Boards where our opponent has fewer legal moves. These are extremely reasonable Board properties to consider in a game where you lose by running out of legal moves.\n",
    "\n",
    "As Dr. Starner mentioned in Lecture 9 of the \"Project: Build a Game-Playing Agent\" videos, one can consider adjust the \"weights\" of the terms in the linear combination of `improved_score`. In particular, he proposed a modified version of `improved_score` that would essentially return `own_moves - 2 * opp_moves`. He claimed that IsolationPlayers using this evaluation function would, on average, play more aggresively, leaning more towards Boards where the opponent has fewer moves, which is ultimately how you win in this game. So that seems like a very reasonable modification to make.\n",
    "\n",
    "But is \"-2\" really the optimal coefficient for `opp_moves`? And are there other features of a Board, aside from `own_moves` and `opp_moves` that would also point our IsolationPlayers towards better board positions? Let's investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2: How Will We Know When We're Done?\n",
    "\n",
    "If the goal is to construct an evaluation function that outperforms `score_improved`, I think it's important to define what we mean by \"outperform.\" Given a new-and-possibly-improved evaluation function `contender`, it seems like I have two options for demonstrating superiority:\n",
    "\n",
    "1. Show, using the tools of math and logic, that `contender` is provably better than score_improved.\n",
    "2. Show empirically that `contender` performs better, on average, than `score_improved`.\n",
    "\n",
    "Option 2 seems much easier, so let's pursue this approach.\n",
    "\n",
    "One reason why Option 2 is easier: the project comes with `tournament.py`, which pits the **test agents** `AB_Improved` (an `AlphaBetaPlayer` using `score_improved`) and one or more `AlphaBetaPlayers` equipped custom evaluation functions against a motley gauntlet of seven opponents: an opponent that moves randomly, three `MinimaxPlayer`s using various evaluation functions, and three `AlphaBetaPlayer`s using various evaluation functions. Overall Win Rates are calculated for each test agent.\n",
    "\n",
    "So that's the plan. Let's see an example of the plan in action:\n",
    "\n",
    "In particular, suppose I had developed nine different evaluation functions using the following pseudocode algorithm:\n",
    "```\n",
    "for own_moves_weight in (1, 2, 3):\n",
    "    for opp_moves_weight in (-3, -2, -1):\n",
    "        modify improved_score to use own_moves_weight and opp_moves_weight\n",
    "```\n",
    "\n",
    "According to the plan, I should equip some `AlphaBetaPlayers` with these evaluation functions and run some tournaments. I did this, and obtained the following output, where the tuple in the test_agent's name indicates the weights used:\n",
    "\n",
    "![Effects of varying opp_moves_weight, with constant own_moves_weight = 1.](report-pics/section2/1-DONE.png)\n",
    "\n",
    "![Effects of varying opp_moves_weight, with constant own_moves_weight = 2.](report-pics/section2/2-DONE.png)\n",
    "\n",
    "![Effects of varying opp_moves_weight, with constant own_moves_weight = 3.](report-pics/section2/3-DONE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So let's see... (1, -1) actually seems like the best combination tested, with a Win Rate of 75.7%. But (3, -2) was also pretty potent, with a Win Rate of 74.3%. And in all three runs, at least one of these weighted test agents outperformed AB_Improved. So give or take some fine-tuning, we've got our improved evaluation functions, right?\n",
    "\n",
    "Maybe, but I'm not convinced, and you shouldn't be either. This is a random process (for example: `AB_Improved` won 67.1% of its games in the first and third tournaments, and only 61.4% in the second tournament). If I were to repeat this entire process with three more tournaments, the win rates could change. How can we be sure that what we're seeing in a given run is signal (an actual difference in performance between evaluation functions) and not noise (random variation)?\n",
    "\n",
    "This is, of course, a statistics problem. The randomness makes it almost impossible to say anything with certainty. But a statistical test would (ideally) let us conclude with high confidence/probability that the true win rate for one of our custom evaluation functions was higher than it was for `score_improved.`\n",
    "\n",
    "I presented this point of view in the [Udacity forums](https://discussions.udacity.com/t/sources-of-variation-in-tournaments-and-why-forfeits/268387/5?u=hha) (that this was inherently a statistical problem, and that we should be thinking about statistical tests), and it was mostly glossed over. The proposed solution to this problem was just to run more matches (i.e. increase the sample size to decrease noise levels for the means), and just eyeball the results for differences.\n",
    "\n",
    "Even if we're not going to perform a formal statistical test, it still makes sense to get a feel for the amount of variability in this process. If `AB_Improved` is always between 61.4% and 67.1% (the values we observed in the three tournaments above), for example, then a `contender` with win rates consistently in the 70+% range would seem to be outperforming `AB_Improved` with high probability.\n",
    "\n",
    "As it turns out, with 5 fair matches per matchup (i.e. 10 games per matchup, as above), there's siginificantly more variability than 61-67%. **Confession time: I didn't actually implement those nine evaluation functions. Every single one of the test agents in the above three tournaments was an `AB_Improved` agent.** So based on those results, we see that with 5 fair matches per matchup, the quantity that we're supposed to be comparing our performance against can be anywhere between about 60% and about 75%. Maybe there's even greater variability that we didn't happen to see in those three runs.\n",
    "\n",
    "That seems like quite a bit of background noise to me. And the more I thought about this, the stranger that seemed to me. After all, this is a fully observable, deterministic, static, discrete environment that we're playing in here. And the algorithms that our AIs are using (aside from the `Random` opponent, who just selects a random legal move on each turn) are almost completely deterministic. Where is all this variation coming from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3: Analysis of Variance\n",
    "\n",
    "First, let's make sure that we can really reduce the variability in Win Rates by increasing the number of fair matches (let's call that `NUM_MATCHES`). Here's a tournament with all four test agents using `score_improved` and `NUM_MATCHES` increased to 25:\n",
    "\n",
    "![Tournament, all test agents using ab_improved, NUM_MATCHES = 25](report-pics/section3/tourn1.png)\n",
    "\n",
    "So indeed, by playing more matches per matchup, the Win Rates are within just a couple percentage points of about 67%, which is the figure that's often reported in the forums for working code.\n",
    "\n",
    "The problem with this approach is that it's computationally expensive. A gridsearch in a multidimensional weight space to find good weights for the various board features in our custom evaluation function(s) could take hours or days. If we need to pay that price, then that's life, but consider this row from that output for a second:\n",
    "\n",
    "![Same evaluation function for all test agents, same starting Board configurations, same opponent, mostly deterministic search algorithms: where is that variation coming from?](report-pics/section3/tourn1-row.png)\n",
    "\n",
    "Why is there so much variation in those outcomes? As I said above, this is a fully observable, deterministic, static, discrete environment, and our agents are using what appear to be mostly deterministic search algorithms to select their moves. \n",
    "\n",
    "Well, note that these things seem to be the same (and therefore can't be causing the observed differences):\n",
    "\n",
    "* The opponent is using the same evaluation function in all four cells\n",
    "* The evaluation function used by the four test agents is exactly the same\n",
    "* They're all using the same `AlphaBetaPlayer` code, and Alpha-Beta search isn't inherently a random algorithm\n",
    "* The initial board configurations are exactly the same for each set of 50 games\n",
    "\n",
    "The only differences I could think of were:\n",
    "\n",
    "1. Differences in depth searched, due to differences in available computing resources. I started that tournament on an otherwise idling computer (with a quad-core processor) and then walked away from it while it ran, but maybe some background task was finishing up while the first test agent played, and then another background task started with the fourth agent was playing, meaning they could search less deeply in the provided time, resulting in their lower win rates.\n",
    "\n",
    "2. The single piece of randomness in my Alpha-Beta implementation: the initialization of `best_move` to a random legal move before the search begins.\n",
    "\n",
    "(This list actually used to have 3 items: I misread the code in `tournament.py` and thought that the test agents could be playing with different initial board configurations, which seemed like a huge potential source of variability in a given row. So I rewrote `tournament.py` to implement what I called \"fair rounds,\" where the same opening moves are used for all test agents' games, only to learn that it already did that. Oops. I will note that the code can be made much cleaner by using `Counter`s to count things.)\n",
    "\n",
    "Controlling for #1 would be doable: we could let the first test agent search as deeply as it could before time ran out, and then limit the rest of the test agents based on depth, not time. \n",
    "\n",
    "But controlling for #2 was laughably easy. While reading the project code, I noticed that `Board.__get_moves` first produces all legal moves in the same order every time, and then shuffles (randomizes) that list of moves before it returns it. So when I wrote my `AlphaBeta` code, I didn't bother to shuffle the result of `Board.get_legal_moves` (as many people in the forums, mentors included, were recommending), because it would just be a complication and a waste of computational resources - the list is already shuffled! Instead, I just initialized `best_move = game.get_legal_moves(self)[0]`, which gets a random legal move if `Board.__get_moves` is shuffling, and **which will return the same move every time, given the board configuration, if that \"shuffle\" line is commented out**. Commenting out that single line of code (`random.shuffle(valid_moves)` in `Board.__get_moves`) and rerunning the tournament produced the following results:\n",
    "\n",
    "![Tournament, all test agents using ab_improved, NUM_MATCHES = 25. Same best_move initialization every time, given the board configuraton.](report-pics/section3/tourn2.png)\n",
    "\n",
    "Note how much variability across the rows has decreased. The main exception to this is `Random`, but this particular opponent obviously has a source of randomness that the `MinimaxPlayer`s and `AlphaBetaPlayer`s don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Q: Why does the best_move initialization matter so much? After all, I have this sweet search algorithm - shouldn't it almost immediately find a better move and overwrite that initial best_move value?**\n",
    "\n",
    "Great question. In game situations where there are at least one decent legal move, the first decent move should result in a board with a much higher score than $-\\infty$, causing `best_move` to be overwritten. But the way that I coded it, `best_move` is only overwritten if a legal move has a score *strictly greater* than the previous `best_move`. And if it's looking bleak for our test agent, in the sense that all roads appear to lead to a loss, then `best_move` will never actually get overwritten. \n",
    "\n",
    "This is the source of many of the forfeits mentioned in the forums: if you initialize `best_move` to `(-1, -1)` and the above effect occurs, then the agent will forfeit as soon as it seems like it's out of options. But immediately forfeiting instead of playing some legal move is a highly suboptimal strategy: this apparent bleakness is a function of the probably-not-perfect evaluation function that the agent is using, so it might not even be an accurate reflection of the situation. And even if it is accurate, playing a legal move gives the opponent an opportunity to forfeit, play a bad move, etc., which could salvage the game for our agent.\n",
    "\n",
    "**Q: Is there a better way to initialize `best_move` than just picking a random legal move? It seems like it gets played a fair amount, so optimizing this could be worth it.**\n",
    "\n",
    "Probably. For example, you could initialize it to the available move that moves you closest to the center of the board, or to the available move that achieves some other short-term goal. I didn't explore this idea, but it's a good one.\n",
    "\n",
    "**Q: Does this decrease in variability hold when we reduce `NUM_MATCHES` back to something that's more computationally tractable?**\n",
    "\n",
    "It certainly seems to:\n",
    "\n",
    "![Tournament, all test agents using ab_improved, NUM_MATCHES = 5, non-random best_move initialization](report-pics/section3/num-matches-5/tourn1.png)\n",
    "\n",
    "![Tournament, all test agents using ab_improved, NUM_MATCHES = 5, non-random best_move initialization](report-pics/section3/num-matches-5/tourn5.png)\n",
    "\n",
    "\n",
    "**Q: Couldn't we make this even more computationally tractable by parallelizing the tournament? Evaluating multiple evaluation functions (in independent matches) seems like an embarassingly parallel problem.**\n",
    "\n",
    "Yes, I think we can. I played around with this idea on my own a little bit before learning that there's a git branch dedicated to parallel tournaments. I started using that, and computational time decreased significantly, as expected.\n",
    "\n",
    "**Q: So what's the plan moving forward?**\n",
    "\n",
    "* The reduction in variation achieved by initializating `best_move` to the first available legal move consistently should make it much easier to visually attribute differences between Win Rate to the evaluation functions used and not to random variation\n",
    "* To further reduce variation, let's **remove `Random` from the gauntlet of opponents**\n",
    "* This reduction in variation seems to hold even when `NUM_MATCHES` is relatively small\n",
    "* And running the tournaments in parallel makes things go faster\n",
    "\n",
    "**So let's run a bunch of tournaments with `AB_Improved` and test agents equipped with different evaluation functions to see if we can find an evaluation function that outperforms `score_improved`!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4: Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As hinted at in the Introductory section of this report, the evaluation functions we try will be linear combinations of various features of the Isolation board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### EVALUATION FUNCTION BUILDING BLOCKS: BOARD FEATURES ###\n",
    "def num_moves(game, player):\n",
    "    \"\"\"How many legal moves does `player` have in `game`?\"\"\"\n",
    "    return len(game.get_legal_moves(player))\n",
    "\n",
    "def num_moves_opponent(game, player):\n",
    "    \"\"\"How many legal moves does the opponent of `player` have in `game`?\"\"\"\n",
    "    return len(game.get_legal_moves(game.get_opponent(player)))\n",
    "\n",
    "def dist_from_center(game, player):\n",
    "    \"\"\"What is `player`'s Manhattan distance from the center of the board?\"\"\"\n",
    "    center_x = game.height / 2\n",
    "    center_y = game.width / 2\n",
    "    player_x, player_y = game.get_player_location(player)\n",
    "    return abs(center_x - player_x) + abs(center_y - player_y)\n",
    "\n",
    "def dist_from_opponent(game, player):\n",
    "    \"\"\"What is `player`'s Manhattan distance from its opponent?\"\"\"\n",
    "    player_x, player_y = game.get_player_location(player)\n",
    "    opponent = game.get_opponent(player)\n",
    "    opp_x, opp_y = game.get_player_location(opponent)\n",
    "    return abs(opp_x - player_x) + abs(opp_y - player_y)\n",
    "\n",
    "HEURISTICS = (num_moves, num_moves_opponent, dist_from_center, dist_from_opponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we'll need a way to generate tournaments Agents that use `AlphaBetaPlayer`s, equipped with linear combinations of these board features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "from game_agent import AlphaBetaPlayer\n",
    "\n",
    "WAgent = namedtuple(\"WAgent\", [\"player\", \"name\", \"weights\"])   # tournament Agent with weights\n",
    "\n",
    "def make_AB_WAgent(weights, fns=HEURISTICS):\n",
    "    \"\"\"Make a WAgent, using a linear combo of `weights` and `fns`.\"\"\"\n",
    "    score_fn = build_weighted_fn(weights, fns)\n",
    "    name = weights_to_string(weights)\n",
    "    return WAgent(AlphaBetaPlayer(score_fn=score_fn), name, weights)\n",
    "\n",
    "# Helper functions\n",
    "def build_weighted_fn(weights, fns):\n",
    "    \"\"\"\n",
    "    Given weights and heuristic functions, create a score function that's a linear\n",
    "    combination those weights and heuristics.\n",
    "    \"\"\"\n",
    "    assert len(weights) == len(fns)\n",
    "    def score_fn(game, player):\n",
    "        if game.is_loser(player):\n",
    "            return float(\"-inf\")\n",
    "        if game.is_winner(player):\n",
    "            return float(\"inf\")\n",
    "        # skip features with weight == 0 to avoid computational time\n",
    "        return sum(w * f(game, player) for w, f in zip(weights, fns) if w)\n",
    "    return score_fn\n",
    "\n",
    "def weights_to_string(weights):\n",
    "    template = \"|\".join([\"{}\"] * len(weights))\n",
    "    return template.format(*weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Which linear combinations should we consider? I think it makes sense to consider positive coefficients for `num_moves` and negative coefficients for `num_moves_opponent`. I'm not sure how `dist_from_center` and `dist_from_opponent` should be treated, so I'll consider both positive and negative coefficients, and I'll make their scales similar so that they can have an equally large impact on the board's score if they have something to say.\n",
    "\n",
    "Here are the coefficient values I'll consider for each board feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "COEFFS = (\n",
    "    (0, 1, 2, 3, 4),       # num_moves\n",
    "    (-4, -3, -2, -1, 0),   # num_moves_opponent\n",
    "    (-4, -2, 0, 2, 4),     # dist_from_center\n",
    "    (-4, -2, 0, 2, 4)      # dist_from_opponent\n",
    ")\n",
    "\n",
    "# How many combinations?\n",
    "reduce(mul, (len(cs) for cs in COEFFS), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's build some agents using our coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def make_all_agents():\n",
    "    for weights in product(*COEFFS):\n",
    "        yield make_AB_WAgent(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And finally, we'll need to enter three of our agents at a time into a tournament with `AB_Improved` and capture the results of the tournament. (It would be straightforward to modify `tournament.py` to accept more than four test agents, but four test agents per tournament will be fine for our purposes.)\n",
    "\n",
    "Unfortunately, `tournament.py` is geared towards printing the results to the terminal, not silently crunching and then saving the results. Fortunately, that's easy enough to implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tournament import Agent\n",
    "from game_agent import custom_score, custom_score_2, custom_score_3, MinimaxPlayer\n",
    "from sample_players import open_move_score, center_score\n",
    "\n",
    "test_agents = [\n",
    "    Agent(AlphaBetaPlayer(score_fn=improved_score), \"AB_Improved\"),\n",
    "    make_AB_WAgent((2, -1, 0, 0)),\n",
    "    make_AB_WAgent((1, -2, 0, 0)),\n",
    "    make_AB_WAgent((2, -3, 1, -1))\n",
    "]\n",
    "\n",
    "cpu_agents = [\n",
    "    Agent(MinimaxPlayer(score_fn=open_move_score), \"MM_Open\"),\n",
    "    Agent(MinimaxPlayer(score_fn=center_score), \"MM_Center\"),\n",
    "    Agent(MinimaxPlayer(score_fn=improved_score), \"MM_Improved\"),\n",
    "    Agent(AlphaBetaPlayer(score_fn=open_move_score), \"AB_Open\"),\n",
    "    Agent(AlphaBetaPlayer(score_fn=center_score), \"AB_Center\"),\n",
    "    Agent(AlphaBetaPlayer(score_fn=improved_score), \"AB_Improved\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from tournament import play_round\n",
    "\n",
    "\n",
    "\n",
    "def run_tournament(cpu_agents, test_agents, num_matches):\n",
    "    \"\"\"\n",
    "    Play the same fair matches between the test agent and each cpu_agent individually.\n",
    "    Returns the total number of games won by each test_agent\n",
    "    Based on code provided by Udacity in tournament.py\n",
    "    \"\"\"\n",
    "    total_wins = Counter()\n",
    "    for agent in cpu_agents:\n",
    "        wins = Counter()\n",
    "        counts = play_round(agent, test_agents, wins, num_matches)\n",
    "        del wins[agent]\n",
    "        print(wins)\n",
    "        total_wins.update(wins)\n",
    "        \n",
    "    print(total_wins)\n",
    "    \n",
    "    # The keys of total_wins are the test_agent.players\n",
    "    # We'd like to group the weights with the number of wins\n",
    "    abi, custom_agents = test_agents[0], test_agents[1:]\n",
    "    abi_wins = total_wins[abi.player]\n",
    "    weight_wins = dict()\n",
    "    for ta in custom_agents:\n",
    "        ta_wins = total_wins[ta.player]\n",
    "        if ta_wins > abi_wins:\n",
    "            weight_wins[ta.weights] = ta_wins\n",
    "            \n",
    "    return weight_wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's see this is action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'build_weighted_fn.<locals>.score_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-66b960dd5d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_tournament\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpu_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-1ef5e8f7119f>\u001b[0m in \u001b[0;36mrun_tournament\u001b[0;34m(cpu_agents, test_agents, num_matches)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcpu_agents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mwins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_matches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mwins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hha/Dropbox/programming/AIND/AIND-Isolation/tournament.py\u001b[0m in \u001b[0;36mplay_round\u001b[0;34m(cpu_agent, test_agents, win_counts, num_matches)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# play all games and tally the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermination\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mwinner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/aind/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/aind/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    383\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                         \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/aind/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/aind/lib/python3.6/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'build_weighted_fn.<locals>.score_fn'"
     ]
    }
   ],
   "source": [
    "run_tournament(cpu_agents, test_agents, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ouch, an `AttributeError` about pickling? A bit of Googling revealed that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
